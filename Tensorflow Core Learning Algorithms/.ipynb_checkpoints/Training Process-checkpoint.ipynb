{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74da7984-ddea-47ac-bc1a-e6e9e3e98600",
   "metadata": {},
   "source": [
    "# The Training Process\n",
    "So, we are almost done preparing our dataset and I feel as though it's a good time to explain how our model is trained. Specifically, how input data is fed to our model.\n",
    "\n",
    "For this specific model data is going to be streamed into it in small batches of 32. This means we will not feed the entire dataset to our model at once, but simply small batches of entries. We will feed these batches to our model multiple times according to the number of epochs.\n",
    "\n",
    "An epoch is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.\n",
    "\n",
    "Ex. if we have 10 ephocs, our model will see the same dataset 10 times.\n",
    "\n",
    "Since we need to feed our data in batches and multiple times, we need to create something called an input function. The input function simply defines how our dataset will be converted into batches at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59c03d7-ead2-4cbc-9aff-586723bd1eba",
   "metadata": {},
   "source": [
    "## Input Function\n",
    "The TensorFlow model we are going to use requires that the data we pass it comes in as a tf.data.Dataset object. This means we must create a input function that can convert our current pandas dataframe into that object.\n",
    "\n",
    "Below you'll see a seemingly complicated input function, this is straight from the TensorFlow documentation (<a>https://www.tensorflow.org/tutorials/estimator/linear</a>). I've commented as much as I can to make it understandble, but you may want to refer to the documentation for a detailed explination of each method.\n",
    "\n",
    "Now first let us input the data from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "948bb8fc-f263-401b-ab81-53424240c03e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as fc\n",
    "from tensorflow.keras.layers import StringLookup, IntegerLookup, Normalization\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Example TensorFlow usage\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "699f11ab-db89-4dd7-8883-9b99629e756d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 627, 2), dtype=float32). Expected shape (None, 2), but input has incompatible shape (None, 627, 2)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 627, 2), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:244\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    242\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 627, 2), dtype=float32). Expected shape (None, 2), but input has incompatible shape (None, 627, 2)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 627, 2), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import StringLookup, Normalization\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the data\n",
    "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')\n",
    "\n",
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "# Convert categorical columns to string type to avoid any type issues\n",
    "dftrain[CATEGORICAL_COLUMNS] = dftrain[CATEGORICAL_COLUMNS].astype(str)\n",
    "dfeval[CATEGORICAL_COLUMNS] = dfeval[CATEGORICAL_COLUMNS].astype(str)\n",
    "\n",
    "# Preprocessing layers\n",
    "preprocessing_layers = {}\n",
    "\n",
    "# Create StringLookup layers for categorical features\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocabulary = dftrain[feature_name].unique().tolist()\n",
    "    preprocessing_layers[feature_name] = StringLookup(vocabulary=vocabulary, output_mode='int')\n",
    "\n",
    "# Create Normalization layers for numeric features\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    normalizer = Normalization()\n",
    "    normalizer.adapt(dftrain[feature_name].values)\n",
    "    preprocessing_layers[feature_name] = normalizer\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess(features, label):\n",
    "    numeric_features = []\n",
    "    for feature_name in NUMERIC_COLUMNS:\n",
    "        numeric_features.append(preprocessing_layers[feature_name](features[feature_name]))\n",
    "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "    return numeric_features, label\n",
    "\n",
    "# Convert the DataFrame to a TensorFlow Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((dict(dftrain), y_train))\n",
    "eval_ds = tf.data.Dataset.from_tensor_slices((dict(dfeval), y_eval))\n",
    "\n",
    "# Apply the preprocessing function to the datasets\n",
    "train_ds = train_ds.map(preprocess)\n",
    "eval_ds = eval_ds.map(preprocess)\n",
    "\n",
    "# Batch the dataset\n",
    "train_ds = train_ds.batch(32)\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(len(NUMERIC_COLUMNS),)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010312ae-c5c4-4cd8-b79e-66c07d61c708",
   "metadata": {},
   "source": [
    "<blockquote>Now, we will write the input function here</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff35722-6ac2-43d8-a71e-33c3c42a5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "#   def input_function():  # inner function, this will be returned\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
    "#     if shuffle:\n",
    "#       ds = ds.shuffle(1000)  # randomize order of data\n",
    "#     ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
    "#     return ds  # return a batch of the dataset\n",
    "#   return input_function  # return a function object for use\n",
    "\n",
    "# train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "# eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e75d23-77d9-4d18-b7ec-3a1bc5c2af40",
   "metadata": {},
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "382c4ceb-ee87-4c27-860a-44b02741ca00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert '2' to a shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m  tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((train_numerical_features_dict, train_numerical_labels))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      2\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((eval_numerical_features_dict, eval_numerical_labels ))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUMERIC_COLUMNS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      6\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m16\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdagrad(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:148\u001b[0m, in \u001b[0;36mInput\u001b[1;34m(shape, batch_size, dtype, sparse, batch_shape, name, tensor, optional)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.Input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.Input\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mInput\u001b[39m(\n\u001b[0;32m     93\u001b[0m     shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    101\u001b[0m ):\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used to instantiate a Keras tensor.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    A Keras tensor is a symbolic tensor-like object, which we augment with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mInputLayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:47\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, shape, batch_size, dtype, sparse, batch_shape, input_tensor, optional, name, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must pass a `shape` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandardize_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m (batch_size,) \u001b[38;5;241m+\u001b[39m shape\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(batch_shape)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:530\u001b[0m, in \u001b[0;36mstandardize_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined shapes are not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(shape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, tf\u001b[38;5;241m.\u001b[39mTensorShape):\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;66;03m# `tf.TensorShape` may contain `Dimension` objects.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;66;03m# We need to convert the items in it to either int or `None`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot convert '2' to a shape."
     ]
    }
   ],
   "source": [
    "dataset =  tf.data.Dataset.from_tensor_slices((train_numerical_features_dict, train_numerical_labels)).batch(32)\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((eval_numerical_features_dict, eval_numerical_labels )).batch(32)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(len(NUMERIC_COLUMNS))),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a30cf622-6eb6-4163-a576-1363315d7357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=({'age': TensorSpec(shape=(None, 627), dtype=tf.float32, name=None), 'fare': TensorSpec(shape=(None, 627), dtype=tf.float32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4ae27f5-c551-4f27-a294-92db09d16516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Load the datasets\n",
    "# dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
    "# dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
    "\n",
    "# # Extract the labels\n",
    "# y_train = dftrain.pop('survived')\n",
    "# y_eval = dfeval.pop('survived')\n",
    "Up\n",
    "# # Define feature columns\n",
    "# CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']\n",
    "# NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "# feature_columns = []\n",
    "# for feature_name in CATEGORICAL_COLUMNS:\n",
    "#     vocabulary = dftrain[feature_name].unique()\n",
    "#     feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
    "\n",
    "# for feature_name in NUMERIC_COLUMNS:\n",
    "#     feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
    "\n",
    "# # Ensure numeric columns are of type float32\n",
    "# dftrain[NUMERIC_COLUMNS] = dftrain[NUMERIC_COLUMNS].astype('float32')\n",
    "# dfeval[NUMERIC_COLUMNS] = dfeval[NUMERIC_COLUMNS].astype('float32')\n",
    "\n",
    "# # Convert categorical columns to string type to avoid any type issues\n",
    "# dftrain[CATEGORICAL_COLUMNS] = dftrain[CATEGORICAL_COLUMNS].astype(str)\n",
    "# dfeval[CATEGORICAL_COLUMNS] = dfeval[CATEGORICAL_COLUMNS].astype(str)\n",
    "\n",
    "# # Create a feature layer\n",
    "# feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "# # Define and compile the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     feature_layer,\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "# # Create datasets\n",
    "# def df_to_dataset(dataframe, labels, shuffle=True, batch_size=32):\n",
    "#     dataframe = dataframe.copy()\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "#     ds = ds.batch(batch_size)\n",
    "#     return ds\n",
    "\n",
    "# batch_size = 32\n",
    "# train_dataset = df_to_dataset(dftrain, y_train, batch_size=batch_size)\n",
    "# eval_dataset = df_to_dataset(dfeval, y_eval, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(train_dataset, epochs=5)\n",
    "\n",
    "# # Evaluate the model\n",
    "# model.evaluate(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60b64a32-a0c8-40a1-bf86-8e4a24a7e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [[1., 1.5], [2., 2.5], [3., 3.5]]\n",
    "# labels = [[0.3], [0.5], [0.7]]\n",
    "# eval_features = [[4., 4.5], [5., 5.5], [6., 6.5]]\n",
    "# eval_labels = [[0.8], [0.9], [1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "079461bb-1c6e-48f5-8ba9-e889ceada0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomModel(tf.keras.Sequential):\n",
    "#   \"\"\"A custom sequential model that overrides `Model.train_step`.\"\"\"\n",
    "\n",
    "#   def train_step(self, data):\n",
    "#     batch_data, labels = data\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#       predictions = self(batch_data, training=True)\n",
    "#       # Compute the loss value (the loss function is configured\n",
    "#       # in `Model.compile`).\n",
    "#       loss = self.compiled_loss(labels, predictions)\n",
    "\n",
    "#     # Compute the gradients of the parameters with respect to the loss.\n",
    "#     gradients = tape.gradient(loss, self.trainable_variables)\n",
    "#     # Perform gradient descent by updating the weights/parameters.\n",
    "#     self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "#     # Update the metrics (includes the metric that tracks the loss).\n",
    "#     self.compiled_metrics.update_state(labels, predictions)\n",
    "#     # Return a dict mapping metric names to the current values.\n",
    "#     return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f912c86-08cd-4cd3-a1ef-c8341eae0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(1)\n",
    "# eval_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "#       (eval_features, eval_labels)).batch(1)\n",
    "\n",
    "# model = CustomModel([tf.keras.layers.Dense(1)])\n",
    "# optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.05)\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73c6ff08-a225-4567-85cc-b5b3197d8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:607: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight, training)`.\n",
      "  warnings.warn(\n",
      "C:\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:582: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
      "```\n",
      "for metric in self.metrics:\n",
      "    metric.update_state(y, y_pred)\n",
      "```\n",
      "\n",
      "  return self._compiled_metrics_update_state(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: -3.3828  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x115d30ea330>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "def dict_to_tuple(features, label):\n",
    "    feature_array = tf.stack([features[feature] for feature in NUMERIC_COLUMNS], axis=-1)\n",
    "    return feature_array, label\n",
    "\n",
    "new_dataset = dataset.map(dict_to_tuple)\n",
    "model.fit(new_dataset,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "603ae3fd-594d-4a5c-87bc-6ddd817442c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 71.7983  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 82.70061492919922}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate(eval_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e680aa-e40d-4c05-ade5-b58f2511817f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
